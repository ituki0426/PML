{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:11:43.913968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 13:11:47.637958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:11:47.637987: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-23 13:11:58.059355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:11:58.059677: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:11:58.059698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-23 13:12:08.231324: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.38 GiB (download: 1.38 GiB, generated: 1.62 GiB, total: 3.00 GiB) to /home/codespace/tensorflow_datasets/celeb_a/2.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 1485204305/1485204305 [00:00<00:00, 19141641415.32 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 46.95 url/s]\n",
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[1;32m      5\u001b[0m celeba_bldr \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mbuilder(\u001b[39m'\u001b[39m\u001b[39mceleb_a\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m celeba_bldr\u001b[39m.\u001b[39;49mdownload_and_prepare()\n\u001b[1;32m      7\u001b[0m celeba \u001b[39m=\u001b[39m celeba_bldr\u001b[39m.\u001b[39mas_dataset(shuffle_files\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m celeba_train \u001b[39m=\u001b[39m celeba[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:628\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    626\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[1;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 628\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    629\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    630\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    631\u001b[0m   )\n\u001b[1;32m    633\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    634\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    635\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    636\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1473\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1461\u001b[0m   \u001b[39mfor\u001b[39;00m split_name, generator \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   1462\u001b[0m       split_generators\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1463\u001b[0m       desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating splits...\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1464\u001b[0m       unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m splits\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1465\u001b[0m       leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m   ):\n\u001b[1;32m   1467\u001b[0m     filename_template \u001b[39m=\u001b[39m naming\u001b[39m.\u001b[39mShardedFileTemplate(\n\u001b[1;32m   1468\u001b[0m         split\u001b[39m=\u001b[39msplit_name,\n\u001b[1;32m   1469\u001b[0m         dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1470\u001b[0m         data_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path,\n\u001b[1;32m   1471\u001b[0m         filetype_suffix\u001b[39m=\u001b[39mpath_suffix,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[0;32m-> 1473\u001b[0m     future \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39;49msubmit_split_generation(\n\u001b[1;32m   1474\u001b[0m         split_name\u001b[39m=\u001b[39;49msplit_name,\n\u001b[1;32m   1475\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m   1476\u001b[0m         filename_template\u001b[39m=\u001b[39;49mfilename_template,\n\u001b[1;32m   1477\u001b[0m         disable_shuffling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mdisable_shuffling,\n\u001b[1;32m   1478\u001b[0m     )\n\u001b[1;32m   1479\u001b[0m     split_info_futures\u001b[39m.\u001b[39mappend(future)\n\u001b[1;32m   1481\u001b[0m \u001b[39m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/split_builder.py:340\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[0;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generator, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n\u001b[0;32m--> 340\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_from_generator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuild_kwargs)\n\u001b[1;32m    341\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Otherwise, beam required\u001b[39;00m\n\u001b[1;32m    342\u001b[0m   unknown_generator_type \u001b[39m=\u001b[39m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    343\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInvalid split generator value for split `\u001b[39m\u001b[39m{\u001b[39;00msplit_name\u001b[39m}\u001b[39;00m\u001b[39m`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    344\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected generator or apache_beam object. Got: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    345\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(generator)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    346\u001b[0m   )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/split_builder.py:417\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[0;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     utils\u001b[39m.\u001b[39mreraise(e, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to encode example:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m   writer\u001b[39m.\u001b[39;49mwrite(key, example)\n\u001b[1;32m    418\u001b[0m shard_lengths, total_size \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39mfinalize()\n\u001b[1;32m    420\u001b[0m split_info \u001b[39m=\u001b[39m splits_lib\u001b[39m.\u001b[39mSplitInfo(\n\u001b[1;32m    421\u001b[0m     name\u001b[39m=\u001b[39msplit_name,\n\u001b[1;32m    422\u001b[0m     shard_lengths\u001b[39m=\u001b[39mshard_lengths,\n\u001b[1;32m    423\u001b[0m     num_bytes\u001b[39m=\u001b[39mtotal_size,\n\u001b[1;32m    424\u001b[0m     filename_template\u001b[39m=\u001b[39mfilename_template,\n\u001b[1;32m    425\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/writer.py:236\u001b[0m, in \u001b[0;36mWriter.write\u001b[0;34m(self, key, example)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, key, example):\n\u001b[1;32m    226\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Writes given Example.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[39m  The given example is not directly written to the tfrecord file, but to a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m    example: the Example to write to the tfrecord file.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m   serialized_example \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serializer\u001b[39m.\u001b[39;49mserialize_example(example\u001b[39m=\u001b[39;49mexample)\n\u001b[1;32m    237\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shuffler\u001b[39m.\u001b[39madd(key, serialized_example)\n\u001b[1;32m    238\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow_datasets/core/example_serializer.py:96\u001b[0m, in \u001b[0;36mExampleSerializer.serialize_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mserialize_example\u001b[39m(\u001b[39mself\u001b[39m, example: TreeDict[Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     86\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Serialize the given example.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m    serialize_proto: `str`, the serialized `tf.train.Example` proto\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_tf_example(example)\u001b[39m.\u001b[39;49mSerializeToString()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "celeba_bldr.download_and_prepare()\n",
    "celeba = celeba_bldr.as_dataset(shuffle_files=True)\n",
    "celeba_train = celeba['train']\n",
    "celeba_valid = celeba['validation']\n",
    "celeba_test = celeba['test']\n",
    "\n",
    "def count_items(ds):\n",
    "    n = 0\n",
    "    for _ in ds:\n",
    "        n += 1\n",
    "    return n\n",
    "    \n",
    "print('Train set: {}'.format(count_items(celeba_train)))\n",
    "print('Validation set: {}'.format(count_items(celeba_valid)))\n",
    "print('Test set: {}'.format(count_items(celeba_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像変換とデータ拡張\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "exmaples = []\n",
    "for example in celeba_train.take(5):\n",
    "    exmaples.append(example['image'])\n",
    "fig = plt.figure(figsize=(16, 8.5))\n",
    "# 列１:画像を短形に切り抜く\n",
    "ax =  fig.add_subplot(2, 5, 1)\n",
    "ax.set_title('Crop to a /nbouding_box', size = 15)\n",
    "ax.imshow(exmaples[0])\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "img_cropped = tf.image.crop_to_bounding_box(exmaples[0], 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "# 列２:画像を水平方向に反転 \n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "ax.set_title('Flip (horizontal)', size = 15)\n",
    "ax.imshow(exmaples[1])\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "img_flipped = tf.image.flip_left_right(exmaples[1])\n",
    "ax.imshow(img_flipped)\n",
    "# 列３:コントラストを調整\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "ax.set_title('Adjust contrast', size = 15)\n",
    "ax.imshow(exmaples[2])\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "img_const = tf.image.adjust_contrast(exmaples[2], 2)\n",
    "ax.imshow(img_const)\n",
    "# 列４:明度を調整する\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "ax.set_title('Adjust brightness', size = 15)\n",
    "ax.imshow(exmaples[3])\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "img_bright = tf.image.adjust_brightness(exmaples[3], 0.4)\n",
    "ax.imshow(img_bright)\n",
    "# 列５:画像を中心で切り抜き、元のサイズ(218x178)にリサイズする\n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "ax.set_title('Center crop', size = 15)\n",
    "ax.imshow(exmaples[4])\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "img_center_crop = tf.image.central_crop(exmaples[4], 0.7)\n",
    "img_resized = tf.image.resize(img_center_crop, size = (218,178))\n",
    "ax.imshow(img_resized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "for i,example in enumerate(celeba_train.take(3)):\n",
    "    image = example['image']\n",
    "    ax = fig.add_subplot(3, 2, 2*i+1)\n",
    "    ax.imshow(image)\n",
    "    if i == 0:\n",
    "        ax.set_title('Original image', size=15)\n",
    "    ax = fig.add_subplot(3, 4, 1*4+2)\n",
    "    img_crop = tf.image.random_crop(image, size = (178, 178, 3))\n",
    "    ax.imshow(img_crop)\n",
    "    if i == 0:\n",
    "        ax.set_title('Random crop', size=15)\n",
    "    ax = fig.add_subplot(3, 4, 2*4+2)\n",
    "    img_flip = tf.image.random_flip_left_right(image)\n",
    "    ax.imshow(img_flip)\n",
    "    if i == 0:\n",
    "        ax.set_title('Random flip', size=15)\n",
    "    ax = fig.add_subplot(3, 4, 3*4+2)\n",
    "    img_contrast = tf.image.random_contrast(image, 0.5, 1.5)\n",
    "    ax.imshow(img_contrast)\n",
    "    if i == 0:\n",
    "        ax.set_title('Random contrast', size=15)\n",
    "    ax = fig.add_subplot(3, 4, 4*4+2)\n",
    "    img_brightness = tf.image.random_brightness(image, 0.5)\n",
    "    ax.imshow(img_brightness)\n",
    "    if i == 0:\n",
    "        ax.set_title('Random brightness', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN性別分類器を訓練する\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same' , activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(rate = 0.5),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding = 'same' , activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(rate = 0.5),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding = 'same' , activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding = 'same' , activation='relu'),\n",
    "])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
